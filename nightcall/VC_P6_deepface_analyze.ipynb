{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4442edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd4a50",
   "metadata": {},
   "source": [
    "# Nightcall Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca5e37",
   "metadata": {},
   "source": [
    "Image Stiching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b1843aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ejemplo 1', 'Ejemplo 2', 'Ejemplo 3', 'Ejemplo 4']\n",
      "Total number of images detected 5\n",
      "Panorama generated\n",
      "Total number of images detected 3\n",
      "Panorama generated\n",
      "Total number of images detected 4\n",
      "Panorama generated\n",
      "Total number of images detected 6\n",
      "Panorama generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "mainFolder = \"Images\"\n",
    "myFolders = os.listdir(mainFolder)\n",
    "print(myFolders)\n",
    "\n",
    "max_width = 800  # Ancho máximo deseado\n",
    "max_height = 600  # Alto máximo deseado\n",
    "\n",
    "for folder in myFolders:\n",
    "    path = mainFolder + '/' + folder\n",
    "    images = []\n",
    "    myList = os.listdir(path)\n",
    "    print(f'Total number of images detected {len(myList)}')\n",
    "    for imgN in myList:\n",
    "        current_img = cv2.imread(f'{path}/{imgN}')\n",
    "        images.append(current_img)\n",
    "    \n",
    "    sticher = cv2.Stitcher.create()\n",
    "    (status, result) = sticher.stitch(images)\n",
    "    \n",
    "    if status == cv2.STITCHER_OK:\n",
    "        print(\"Panorama generated\")\n",
    "        if result.shape[1] > max_width or result.shape[0] > max_height:\n",
    "            # Si es muy grande, reducir su tamaño\n",
    "            result = cv2.resize(result, (max_width, max_height))\n",
    "        \n",
    "        cv2.namedWindow(folder, cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(folder, result)\n",
    "        cv2.waitKey(1)\n",
    "    else:\n",
    "        print(\"Panorama generation unsuccessful\")\n",
    "\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204665d",
   "metadata": {},
   "source": [
    "Colorear imagenes blanco y negro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4abbda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\albert-einstein-1933340_1280.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\canteras.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\girasol.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\landing_craft_omaha_beach_normandy_d_day_war_vintage_france_military-772047.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\mary.jpg\n",
      "Colorizando la imagen\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Rutas para cargar el modelo y la carpeta de imágenes\n",
    "DIR = r\"C:\\Users\\Eric\\Desktop\\nightcall\"\n",
    "PROTOTXT = os.path.join(DIR, r\"model/colorization_deploy_v2.prototxt\")\n",
    "POINTS = os.path.join(DIR, r\"model/pts_in_hull.npy\")\n",
    "MODEL = os.path.join(DIR, r\"model/colorization_release_v2.caffemodel\")\n",
    "IMAGE_FOLDER = r\"C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\"\n",
    "\n",
    "# Cargar el modelo\n",
    "print(\"Cargando modelo\")\n",
    "net = cv2.dnn.readNetFromCaffe(PROTOTXT, MODEL)\n",
    "pts = np.load(POINTS)\n",
    "\n",
    "# Ajustar el modelo para el rebalanceo del canal ab.\n",
    "class8 = net.getLayerId(\"class8_ab\")\n",
    "conv8 = net.getLayerId(\"conv8_313_rh\")\n",
    "pts = pts.transpose().reshape(2, 313, 1, 1)\n",
    "net.getLayer(class8).blobs = [pts.astype(\"float32\")]\n",
    "net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]\n",
    "\n",
    "# Obtener la lista de archivos de imagen en la carpeta especificada\n",
    "image_files = [os.path.join(IMAGE_FOLDER, file) for file in os.listdir(IMAGE_FOLDER) if file.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    # Cargar la imagen de escala de grises\n",
    "    print(f\"Procesando imagen: {image_file}\")\n",
    "    image = cv2.imread(image_file)\n",
    "    scaled = image.astype(\"float32\") / 255.0\n",
    "    lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    resized = cv2.resize(lab, (224, 224))\n",
    "    L = cv2.split(resized)[0]\n",
    "    L -= 50\n",
    "\n",
    "    # Colorizar la imagen\n",
    "    print(\"Colorizando la imagen\")\n",
    "    net.setInput(cv2.dnn.blobFromImage(L))\n",
    "    ab = net.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "    ab = cv2.resize(ab, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    L = cv2.split(lab)[0]\n",
    "    colorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2)\n",
    "    colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)\n",
    "    colorized = np.clip(colorized, 0, 1)\n",
    "    colorized = (255 * colorized).astype(\"uint8\")\n",
    "\n",
    "    # Redimensionar la imagen coloreada para mostrarla en una ventana más pequeña\n",
    "    small_colorized = cv2.resize(colorized, (600, 600))\n",
    "    small_black = cv2.resize(colorized, (600, 600))\n",
    "\n",
    "    # Mostrar las imágenes originales y coloreadas\n",
    "    cv2.imshow(\"Original\", cv2.resize(image, (600, 600)))\n",
    "    cv2.imshow(\"Colorizada (Redimensionada)\", small_colorized)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff865c01",
   "metadata": {},
   "source": [
    "Traductor lengua de signos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1fba5f",
   "metadata": {},
   "source": [
    "IA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff9463",
   "metadata": {},
   "source": [
    "Tienes que preparar los paths de las imagenes con las que entrenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])                 \n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f70ed",
   "metadata": {},
   "source": [
    "Lo que hacemos pa entrenar es lo siguiente, sacar las imagenes de entrenamiento en vivo y en directo en funcion a las acciones que queremos y el numero de imagenes de entrenamiento, que como ves arriba son 30 si quieres más haz más yo ahí no me meto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int))\n",
    "    for sequence in range(1,no_sequences+1):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce693236",
   "metadata": {},
   "source": [
    "Las imagenes en directo de donde extraera los puntos de entrenamiento se pillan en este fragmento de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be573cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, start_folder+no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6cc46",
   "metadata": {},
   "source": [
    "Arquitectura de datos (preprocesamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebcb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4893762",
   "metadata": {},
   "source": [
    "Arquitectura de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee003ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1145c6",
   "metadata": {},
   "source": [
    "Y te escribiria el resto pero es bastante obvio, tienes que sacar predicciones y guardar los pesos. Testeas la red con las predicciones y si ves que atina, guardas los pesos en un .h5 (Haz una matriz de confusión, te ayudará)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019d841",
   "metadata": {},
   "source": [
    "No toques nada de abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a7e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 1s 611ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "\n",
    "#---------------------------------------Extraer puntos clave--------------------------------------------------------\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "#---------------------------------------Cargamos el modelo--------------------------------------------------------\n",
    "\n",
    "\n",
    "model = load_model('action.h5')\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "\n",
    "#---------------------------------------DETECTAR EN TIEMPO REAL--------------------------------------------------------\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "    \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "            \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "   \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "12028effb1af0cd2244438ff9b17d06bb1d7695ec7a554a144e43ec4b8b79006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

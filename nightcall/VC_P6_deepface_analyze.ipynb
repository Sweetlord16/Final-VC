{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4442edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd4a50",
   "metadata": {},
   "source": [
    "# Nightcall Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca5e37",
   "metadata": {},
   "source": [
    "Image Stiching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1843aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ejemplo 1', 'Ejemplo 2', 'Ejemplo 3', 'Ejemplo 4']\n",
      "Total number of images detected 5\n",
      "Panorama generated\n",
      "Total number of images detected 3\n",
      "Panorama generated\n",
      "Total number of images detected 4\n",
      "Panorama generated\n",
      "Total number of images detected 6\n",
      "Panorama generated\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "mainFolder = \"Images\"\n",
    "myFolders = os.listdir(mainFolder)\n",
    "print(myFolders)\n",
    "\n",
    "max_width = 800  # Ancho máximo deseado\n",
    "max_height = 600  # Alto máximo deseado\n",
    "\n",
    "for folder in myFolders:\n",
    "    path = mainFolder + '/' + folder\n",
    "    images = []\n",
    "    myList = os.listdir(path)\n",
    "    print(f'Total number of images detected {len(myList)}')\n",
    "    for imgN in myList:\n",
    "        current_img = cv2.imread(f'{path}/{imgN}')\n",
    "        images.append(current_img)\n",
    "    \n",
    "    sticher = cv2.Stitcher.create()\n",
    "    (status, result) = sticher.stitch(images)\n",
    "    \n",
    "    if status == cv2.STITCHER_OK:\n",
    "        print(\"Panorama generated\")\n",
    "        if result.shape[1] > max_width or result.shape[0] > max_height:\n",
    "            # Si es muy grande, reducir su tamaño\n",
    "            result = cv2.resize(result, (max_width, max_height))\n",
    "        \n",
    "        cv2.namedWindow(folder, cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(folder, result)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print(\"Panorama generation unsuccessful\")\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204665d",
   "metadata": {},
   "source": [
    "Colorear imagenes blanco y negro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4abbda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\albert-einstein-1933340_1280.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\canteras.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\girasol.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\landing_craft_omaha_beach_normandy_d_day_war_vintage_france_military-772047.jpg\n",
      "Colorizando la imagen\n",
      "Procesando imagen: C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\\mary.jpg\n",
      "Colorizando la imagen\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Directorio principal y rutas de archivos\n",
    "directorio_principal = r\"C:\\Users\\Eric\\Desktop\\nightcall\"\n",
    "prototxt_modelo = os.path.join(directorio_principal, r\"model/colorization_deploy_v2.prototxt\")\n",
    "archivo_puntos = os.path.join(directorio_principal, r\"model/pts_in_hull.npy\")\n",
    "caffemodel_modelo = os.path.join(directorio_principal, r\"model/colorization_release_v2.caffemodel\")\n",
    "carpeta_imagenes = r\"C:\\Users\\Eric\\Desktop\\nightcall\\Images_BlackWhite\"\n",
    "\n",
    "# Cargar el modelo\n",
    "print(\"Iniciando la carga del modelo...\")\n",
    "red_neuronal = cv2.dnn.readNetFromCaffe(prototxt_modelo, caffemodel_modelo)\n",
    "puntos = np.load(archivo_puntos)\n",
    "\n",
    "# Ajustar el modelo para equilibrar el canal AB\n",
    "capa_clase = red_neuronal.getLayerId(\"class8_ab\")\n",
    "capa_conv = red_neuronal.getLayerId(\"conv8_313_rh\")\n",
    "puntos = puntos.transpose().reshape(2, 313, 1, 1)\n",
    "red_neuronal.getLayer(capa_clase).blobs = [puntos.astype(\"float32\")]\n",
    "red_neuronal.getLayer(capa_conv).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]\n",
    "\n",
    "# Obtener la lista de archivos de imagen en la carpeta especificada\n",
    "archivos_imagen = [os.path.join(carpeta_imagenes, archivo) for archivo in os.listdir(carpeta_imagenes) if archivo.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "for archivo_imagen in archivos_imagen:\n",
    "    # Cargar la imagen en escala de grises\n",
    "    print(f\"Procesando la imagen: {archivo_imagen}\")\n",
    "    imagen = cv2.imread(archivo_imagen)\n",
    "    imagen_escalada = imagen.astype(\"float32\") / 255.0\n",
    "    imagen_lab = cv2.cvtColor(imagen_escalada, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    imagen_redimensionada = cv2.resize(imagen_lab, (224, 224))\n",
    "    canal_l = cv2.split(imagen_redimensionada)[0]\n",
    "    canal_l -= 50\n",
    "\n",
    "    # Colorizar la imagen\n",
    "    print(\"Colorizando la imagen...\")\n",
    "    red_neuronal.setInput(cv2.dnn.blobFromImage(canal_l))\n",
    "    valores_ab = red_neuronal.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "    valores_ab = cv2.resize(valores_ab, (imagen.shape[1], imagen.shape[0]))\n",
    "\n",
    "    canal_l = cv2.split(imagen_lab)[0]\n",
    "    imagen_colorizada = np.concatenate((canal_l[:, :, np.newaxis], valores_ab), axis=2)\n",
    "    imagen_colorizada = cv2.cvtColor(imagen_colorizada, cv2.COLOR_LAB2BGR)\n",
    "    imagen_colorizada = np.clip(imagen_colorizada, 0, 1)\n",
    "    imagen_colorizada = (255 * imagen_colorizada).astype(\"uint8\")\n",
    "\n",
    "    # Redimensionar la imagen coloreada para mostrarla en una ventana más pequeña\n",
    "    imagen_colorizada_pequena = cv2.resize(imagen_colorizada, (600, 600))\n",
    "    imagen_original_pequena = cv2.resize(imagen_colorizada, (600, 600))\n",
    "\n",
    "    # Mostrar las imágenes originales y coloreadas\n",
    "    cv2.imshow(\"Original\", cv2.resize(imagen, (600, 600)))\n",
    "    cv2.imshow(\"Colorizada (Redimensionada)\", imagen_colorizada_pequena)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff865c01",
   "metadata": {},
   "source": [
    "Traductor lengua de signos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a7e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 1s 611ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "iloveyou\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "thanks\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Configuración de Mediapipe\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Función para realizar la detección mediante Mediapipe\n",
    "def detectar_mediapipe(image, model):\n",
    "    # Convertir la imagen a formato RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    # Procesar la imagen con el modelo\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    # Convertir de nuevo a formato BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "# Función para dibujar los puntos clave de manera estilizada\n",
    "def dibujar_puntos_clave(image, results):\n",
    "    # Dibujar conexiones faciales con estilos\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Dibujar conexiones de postura con estilos\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Dibujar conexiones de la mano izquierda con estilos\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Dibujar conexiones de la mano derecha con estilos  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "# Función para visualizar las probabilidades de las acciones\n",
    "def visualizar_probabilidades(res, acciones, input_frame, colores):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colores[num], -1)\n",
    "        cv2.putText(output_frame, acciones[num], (0, 85+num*40), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "\n",
    "# Función para extraer puntos clave\n",
    "def extraer_puntos_clave(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "# Cargar modelo y definir acciones\n",
    "modelo = load_model('action.h5')\n",
    "acciones = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Configuración para la detección en tiempo real\n",
    "colores = [(52, 168, 83), (232, 17, 35), (86, 35, 232)]\n",
    "secuencia = []\n",
    "frase = []\n",
    "predicciones = []\n",
    "umbral = 0.5\n",
    "\n",
    "# Inicializar la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Configurar el modelo Mediapipe\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Leer el fotograma de la cámara\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Realizar la detección mediante Mediapipe\n",
    "        imagen, resultados = detectar_mediapipe(frame, holistic)\n",
    "        \n",
    "        # Dibujar puntos clave de manera estilizada\n",
    "        dibujar_puntos_clave(imagen, resultados)\n",
    "        \n",
    "        # Lógica de predicción\n",
    "        puntos_clave = extraer_puntos_clave(resultados)\n",
    "        secuencia.append(puntos_clave)\n",
    "        secuencia = secuencia[-30:]\n",
    "            \n",
    "        if len(secuencia) == 30:\n",
    "            res = modelo.predict(np.expand_dims(secuencia, axis=0))[0]\n",
    "            predicciones.append(np.argmax(res))\n",
    "   \n",
    "            if np.unique(predicciones[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > umbral: \n",
    "                    if len(frase) > 0: \n",
    "                        if acciones[np.argmax(res)] != frase[-1]:\n",
    "                            frase.append(acciones[np.argmax(res)])\n",
    "                    else:\n",
    "                        frase.append(acciones[np.argmax(res)])\n",
    "\n",
    "            if len(frase) > 5: \n",
    "                frase = frase[-5:]\n",
    "\n",
    "            # Visualizar probabilidades\n",
    "            imagen = visualizar_probabilidades(res, acciones, imagen, colores)\n",
    "            \n",
    "        # Actualizar la visualización de la frase en la pantalla\n",
    "        cv2.rectangle(imagen, (0, 0), (640, 40), (52, 168, 83), -1)  # Cambiar el color del rectángulo\n",
    "        cv2.putText(imagen, ' '.join(frase), (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 1.5, (0, 0, 0), 3, cv2.LINE_AA)  # Cambiar la fuente, tamaño, color y grosor del texto\n",
    "\n",
    "        \n",
    "        # Mostrar el fotograma en la ventana\n",
    "        cv2.imshow('OpenCV Feed', imagen)\n",
    "\n",
    "        # Terminar el bucle al presionar 'q'\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Liberar la cámara y cerrar la ventana\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('deepface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "12028effb1af0cd2244438ff9b17d06bb1d7695ec7a554a144e43ec4b8b79006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
